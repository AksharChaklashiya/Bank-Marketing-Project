{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fcd2e48",
   "metadata": {},
   "source": [
    "# Bank Marketing Campaign "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274589a",
   "metadata": {},
   "source": [
    "**The aim of this notebook is to build a machine learning model to help ABC Bank identify the right customers to target\n",
    "for their marketing campaign by classification of people buying term deposit or not.**\n",
    "\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "\n",
    "In this notebook, we will go through the following steps. \n",
    "\n",
    "1. Data Preparation\n",
    "   \n",
    "   - Importing libraries\n",
    "   - Data Ingestion\n",
    "   - Data Overview/(Data Intake)\n",
    "   - Categorical & Numerical variables\n",
    "   \n",
    "\n",
    "2. **Model Preparation**\n",
    "\n",
    "\n",
    "3. **Model Building**\n",
    "            \n",
    "    \n",
    "4. Communication\n",
    "\n",
    "==================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72da3f25",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a5451",
   "metadata": {},
   "source": [
    " ## Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e244529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# from feature-engine\n",
    "from feature_engine.imputation import (\n",
    "    AddMissingIndicator,\n",
    "    MeanMedianImputer,\n",
    "    CategoricalImputer,\n",
    ")\n",
    "\n",
    "from feature_engine.transformation import (\n",
    "    LogTransformer,\n",
    "    YeoJohnsonTransformer,\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d17344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4059cf",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e0972e",
   "metadata": {},
   "source": [
    "### Testutility file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8c8382e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing testutility.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile testutility.py\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import yaml \n",
    "import pandas as pd\n",
    "import datetime\n",
    "import gc\n",
    "import re\n",
    "\n",
    "#################\n",
    "# File Reading #\n",
    "#################\n",
    "\n",
    "def read_config_file(filepath):\n",
    "  with open(filepath, \"r\") as stream:\n",
    "    try:\n",
    "      return yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "      logging.error(exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a1568f",
   "metadata": {},
   "source": [
    "### Yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c5b7f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing file.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile file.yaml\n",
    "file_type: csv\n",
    "dataset_name: testfile\n",
    "file_name: bank-additional-full\n",
    "table_name: edsurv\n",
    "inbound_delimeter: \";\"\n",
    "outbound_delimeter: \"|\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0794eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Config file\n",
    "import testutility as util\n",
    "config_data = util.read_config_file(\"file.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68021aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bank-additional-full'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_data[\"file_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c4ab7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './bank-additional-full.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m source_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m config_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(\"\", source_file)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minbound_delimeter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './bank-additional-full.csv'"
     ]
    }
   ],
   "source": [
    "# read the files using config file\n",
    "file_type = config_data[\"file_type\"]\n",
    "source_file = \"./\" + config_data[\"file_name\"] + f\".{file_type}\"\n",
    "\n",
    "# print(\"\", source_file)\n",
    "df = pd.read_csv(source_file, config_data[\"inbound_delimeter\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23427976",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a083895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c8e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30420a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9286d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def summary(df, source_file):\n",
    "    rows = len(df)\n",
    "    columns = len(df.columns)\n",
    "    print(f\"Number of Rows: {rows}\")\n",
    "    print(f\"Number of Columns: {columns}\")\n",
    "    file_size = os.path.getsize(source_file)\n",
    "    print(f\"Size: {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0822f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(df, source_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c136ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98703e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7019f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585911e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3f52a",
   "metadata": {},
   "source": [
    "Data types are correctly assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a42501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f71741",
   "metadata": {},
   "source": [
    "Now, as per above result,  we can see there is no any duplicate value nor null value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f5b39",
   "metadata": {},
   "source": [
    "## Categorical & Numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953fe25",
   "metadata": {},
   "source": [
    "### Variable Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd98e5",
   "metadata": {},
   "source": [
    "#### Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e8bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var = ['y']\n",
    "target = df[target_var]\n",
    "target_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caecf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"y\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a61940d",
   "metadata": {},
   "source": [
    "#### Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9be65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_var = [var for var in df.columns \n",
    "                   if df[var].dtype==\"O\" and \n",
    "                   var not in target_var and \n",
    "                   var not in [\"month\", \"day_of_week\", \"default\"]]\n",
    "categorical_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore the values of these categorical variables\n",
    "for var in categorical_var:\n",
    "    print(var, df[var].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd28f7",
   "metadata": {},
   "source": [
    "#### Numeric Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4637a974",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_var = [var for var in df.columns if var not in categorical_var + target_var and var not in [\"month\", \"day_of_week\"]]\n",
    "numerical_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c058388",
   "metadata": {},
   "source": [
    "#### Temporal Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d6901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_var = [var for var in df.columns if var ==\"month\" or var==\"day_of_week\"]\n",
    "temporal_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fc7f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore the values of these temporal variables\n",
    "\n",
    "for var in temporal_var:\n",
    "    print(var, df[var].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48fe3f4",
   "metadata": {},
   "source": [
    "#### Discreete Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc3fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_var = [var for var in numerical_var if len(df[var].unique()) < 32 and var not in temporal_var]\n",
    "discrete_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore the values of these discrete variables\n",
    "\n",
    "for var in discrete_var:\n",
    "    print(var, df[var].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24af25",
   "metadata": {},
   "source": [
    "##### Continuous variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb8814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of continuous variables\n",
    "continuous_var = [\n",
    "    var for var in numerical_var if var not in discrete_var+temporal_var]\n",
    "continuous_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbedb36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore the values of these temporal variables\n",
    "\n",
    "for var in continuous_var:\n",
    "    print(var, df[var].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d528948",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a6b8a",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "- Duration is a variable we can explore during EDA for business purposes to decide if we should try engaging the person for longer time on the call or not. The model should be built with and without the duration variable becuase Duration is obtained after the call is made to the potential client so if the target client has never received calls, this feature is not very useful. \n",
    "- No missing values or duplicate values. Data Types correctly assigned. So no need of change there.\n",
    "- There is a 999 value existing in pdays column meaning the customer has not been contacted before. We should change it to 0\n",
    "- Outliers need to be handled in the deposit column\n",
    "- Age column can be log transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df57ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping default column since very few values with \"yes\" so do not give model any predictive power\n",
    "df.drop(columns =['default'] ,axis =1 ,inplace = True)\n",
    "df.drop(columns = [\"duration\"]), axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a06962c",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"y\"\n",
    "X = df.drop(columns=target)\n",
    "y = df[target]\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3862d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c4648b",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a82e6bc",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b1e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf9ab3c",
   "metadata": {},
   "source": [
    "Fortunately, there are no missing values in the dataset. However, after exploring we find that 6 of the categorical variables have an \"unknown\" value. Those are the only missing values which do not necessarily need to be dealt with as the \"unknown\" category is already created for them. However, the categorical variables with a small proportion of \"unknown\" values (less than 10%) can be replaced with the most frequent value of the column while the ones with high proportion of missing values (more than 10%) can be left as a separate \"unknown\" category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb0197",
   "metadata": {},
   "source": [
    "Since we removed the default column there are only 5 categorical variables with unknown values and none of them have more than 10% unknown value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132a5178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To handle the missing values, we first turn the 'unknown' observations to NaNs\n",
    "X_train = X_train.replace('unknown', np.nan)\n",
    "X_test = X_test.replace('unknown', np.nan)\n",
    "X_train.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e507682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of the categorical variables that contain missing values\n",
    "\n",
    "cat_vars_with_na = [\n",
    "    var for var in categorical_var\n",
    "    if X_train[var].isnull().sum() > 0\n",
    "]\n",
    "\n",
    "# print percentage of missing values per variable\n",
    "X_train[cat_vars_with_na ].isnull().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04508e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars_with_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # variables to impute with the string \"unknown\"\n",
    "# with_string_missing = [\n",
    "#     var for var in cat_vars_with_na if X_train[var].isnull().mean() > 0.05]\n",
    "\n",
    "# variables to impute with the most frequent category\n",
    "with_frequent_category = [\n",
    "    var for var in cat_vars_with_na if X_train[var].isnull().mean() < 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfeed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I print the values here, because it makes it easier for\n",
    "# # later when we need to add this values to a config file for \n",
    "# # deployment\n",
    "\n",
    "# with_string_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e302513",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_frequent_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2363e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # replace missing values with new label: \"unknown\"\n",
    "\n",
    "# # set up the class\n",
    "# cat_imputer_missing = CategoricalImputer(\n",
    "#     imputation_method='missing', fill_value = \"unknown\", variables=with_string_missing)\n",
    "\n",
    "# # fit the class to the train set\n",
    "# cat_imputer_missing.fit(X_train)\n",
    "\n",
    "# # the class learns and stores the parameters\n",
    "# cat_imputer_missing.imputer_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849b27f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # replace NA by missing\n",
    "# X_train = cat_imputer_missing.transform(X_train)\n",
    "# X_test = cat_imputer_missing.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values with most frequent category\n",
    "\n",
    "# set up the class\n",
    "cat_imputer_frequent = CategoricalImputer(\n",
    "    imputation_method='frequent', variables=with_frequent_category)\n",
    "\n",
    "# fit the class to the train set\n",
    "cat_imputer_frequent.fit(X_train)\n",
    "\n",
    "# the class learns and stores the parameters\n",
    "cat_imputer_frequent.imputer_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14076791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NA by missing\n",
    "X_train = cat_imputer_frequent.transform(X_train)\n",
    "X_test = cat_imputer_frequent.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd76d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we have no missing information in the engineered variables\n",
    "\n",
    "X_train[cat_vars_with_na].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32272a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that test set does not contain null values in the engineered variables\n",
    "\n",
    "[var for var in cat_vars_with_na if X_test[var].isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24aa49d",
   "metadata": {},
   "source": [
    "## Numerical variable transformation\n",
    "\n",
    "### Logarithmic transformation\n",
    "\n",
    "In the previous notebook, we observed that the numerical variables are not normally distributed.\n",
    "\n",
    "We will transform with the logarightm the \"age\" variable in order to get a more Gaussian-like distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e14296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transformer = LogTransformer(\n",
    "    variables=[\"age\"])\n",
    "\n",
    "X_train = log_transformer.fit_transform(X_train)\n",
    "X_test = log_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83344b8c",
   "metadata": {},
   "source": [
    "### Yeo-Johnson transformation\n",
    "\n",
    "We will apply the Yeo-Johnson transformation to duration to obtain gaussian like distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d1572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeo_transformer = YeoJohnsonTransformer(\n",
    "#     variables=['duration'])\n",
    "\n",
    "# X_train = yeo_transformer.fit_transform(X_train)\n",
    "# X_test = yeo_transformer.transform(X_test)\n",
    "\n",
    "# # the learned parameter\n",
    "# yeo_transformer.lambda_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af7645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f8af0c",
   "metadata": {},
   "source": [
    "## Categorical feature encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecf52fa",
   "metadata": {},
   "source": [
    "### Replace Binary Features with 1 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4e7eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_features = ['housing', 'loan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b9b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in binary_features:\n",
    "    X_train[col] = X_train[col].replace([\"yes\"], 1)\n",
    "    X_train[col] = X_train[col].replace([\"no\"], 0)\n",
    "    X_test[col] = X_test[col].replace([\"yes\"], 1)\n",
    "    X_test[col] = X_test[col].replace([\"no\"], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f02250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379e0fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ord_enc = OrdinalEncoder(categories = [['basic.9y', 'university.degree', 'illiterate', 'basic.4y', 'basic.6y', 'professional.course', 'high.school']])\n",
    "X_train.education = ord_enc.fit_transform(X_train.loc[:, [\"education\"]])\n",
    "X_test.education = ord_enc.transform(X_test.loc[:, [\"education\"]])\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f85b332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "y_train = y_train.to_frame()\n",
    "y_test = y_test.to_frame()\n",
    "y_train[\"y\"]=le.fit_transform(y_train[\"y\"])\n",
    "y_test[\"y\"] = le.transform(y_test[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfcf06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8986aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b906112",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_encode_Xtrain = X_train.select_dtypes(include=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2144db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_encode_Xtest = X_test.select_dtypes(include=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ddea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the variables to encode first\n",
    "\n",
    "for col in cols_to_encode_Xtrain:\n",
    "  X_train = pd.concat([X_train, pd.get_dummies(X_train[col], prefix=\"%s\"%col)], axis=1)\n",
    "  X_train.drop([col], axis=1, inplace=True)\n",
    "\n",
    "for col in cols_to_encode_Xtest:\n",
    "  X_test = pd.concat([X_test, pd.get_dummies(X_test[col], prefix=\"%s\"%col)], axis=1)\n",
    "  X_test.drop([col], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f95da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc42fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9fdb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8241f11f",
   "metadata": {},
   "source": [
    "### Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dee5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "over_sampler = RandomOverSampler(random_state=42)\n",
    "X_train_over, y_train_over = over_sampler.fit_resample(X_train,y_train)\n",
    "print(\"X_train_over shape:\", X_train_over.shape)\n",
    "X_train_over.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf5d3c",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc821e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_baseline = y_train.value_counts(normalize=True).max()\n",
    "print(\"Baseline Accuracy:\", round(acc_baseline, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602f9664",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(\n",
    "    RandomForestClassifier(random_state=42)\n",
    ")\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed9c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_acc_scores = cross_val_score(clf, X_train_over, y_train_over, cv=5, n_jobs=-1)\n",
    "print(cv_acc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"randomforestclassifier__n_estimators\": range(25,100,25),\n",
    "    \"randomforestclassifier__max_depth\": range(10,50,10),\n",
    "\n",
    "}\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f60cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GridSearchCV(\n",
    "    clf,\n",
    "    param_grid = params,\n",
    "    cv = 5,\n",
    "    n_jobs = -1,\n",
    "    verbose= 1\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2432749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851ff6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(model.cv_results_)\n",
    "cv_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7f9c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mask\n",
    "mask = cv_results[\"param_randomforestclassifier__max_depth\"] == 40\n",
    "# Plot fit time vs n_estimators\n",
    "plt.plot( cv_results[mask][\"param_randomforestclassifier__n_estimators\"],\n",
    "         cv_results[mask][\"mean_fit_time\"]    \n",
    ")\n",
    "# Label axes\n",
    "plt.xlabel(\"Number of Estimators\")\n",
    "plt.ylabel(\"Mean Fit Time [seconds]\")\n",
    "plt.title(\"Training Time vs Estimators (max_depth=40)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c38cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mask\n",
    "mask = cv_results[\"param_randomforestclassifier__n_estimators\"] == 50\n",
    "# Plot fit time vs max_depth\n",
    "plt.plot( cv_results[mask][\"param_randomforestclassifier__max_depth\"],\n",
    "         cv_results[mask][\"mean_fit_time\"]\n",
    "    \n",
    ")\n",
    "\n",
    "# Label axes\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.ylabel(\"Mean Fit Time [seconds]\")\n",
    "plt.title(\"Training Time vs Max Depth (n_estimators=50)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209de0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[mask][[\"mean_fit_time\", \"param_randomforestclassifier__max_depth\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best hyperparameters\n",
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764ef2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d57a4",
   "metadata": {},
   "source": [
    "### Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd53c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a382d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = model.score(X_train_over,y_train_over)\n",
    "acc_test = model.score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training Accuracy:\", round(acc_train, 4))\n",
    "print(\"Test Accuracy:\", round(acc_test, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d938248",
   "metadata": {},
   "source": [
    "We beat the baseline! Just barely, but we beat it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad639bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb56a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "# predict probabilities\n",
    "lr_probs = model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('RabdomForest: ROC AUC=%.3f' % (lr_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "# plot the roc curve for the model\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', label='Random Forest')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf27a41",
   "metadata": {},
   "source": [
    "### Communicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95562d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names from training data\n",
    "features = X_train_over.columns\n",
    "\n",
    "# Extract importances from model\n",
    "importances = model.best_estimator_.named_steps[\"randomforestclassifier\"].feature_importances_\n",
    "# Create a series with feature names and importances\n",
    "feat_imp = pd.Series(importances, index=features).sort_values()\n",
    "# Plot 10 most important features\n",
    "feat_imp.tail(10).plot(kind = \"barh\")\n",
    "plt.xlabel(\"Gini Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataGlacier_Project",
   "language": "python",
   "name": "dataglacier_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
